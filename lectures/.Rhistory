dataout$srate<-NA
dataout$B<-NA
dataout$Nstar<-NA
dataout$Bstar<-NA
dataout$Relpop<-NA
dataout$Relt_1<-NA
dataout$RelB<-NA
dataout$Species<-NA
dataout$deltabrate<-NA
dataout$deltaB<-NA
dataout$deltaN<-NA
####Loop over all datasets####
for (i in 1:max(x$Dataset)){
agedata<-x[which(x$Dataset==i),]#extract individual dataset
agedata$firstsurvival<-min(agedata$CensusOffset)/agedata$Interval[1] ###Correction needed for offset between decade collected and last decade of observed establishment
i10<-(agedata$firstsurvival[1]*agedata$Interval[1])
if (i10<=agint) {surv1<- svec[which(TimeSh==max(agedata$Talign)+agedata$Interval[1])]^(i10/agint)}
if (i10>agint) { wsteps<-floor(i10/agint) ###Whole time steps missing in 5 year periods
psteps<-(i10/agint)-wsteps ###partial time steps missing
steps<-ceiling(i10/agint) ##Total number of steps, rounded up
surv1<-prod(svec[(which(TimeSh==max(agedata$Talign)+agedata$Interval[1])):(which(TimeSh==max(agedata$Talign)+agedata$Interval[1])+wsteps-1)])*(svec[(which(TimeSh==max(agedata$Talign)+agedata$Interval[1])+steps-1)]^(psteps)) ##survival from the last establishment cohort to census
}
srateint<-numeric(length(agedata$B)+1)# Create vector of survival values for each interval. It's 1 longer than the number of intervals to include survival from the last cohort to census
srateint[length(agedata$B)+1]<-surv1 ###last slot is survival from the last cohort to census
for(ii in (length(agedata$B)):1){
srateint[ii]<-svec[which(TimeSh>=agedata$Talign[ii] & TimeSh<agedata$Talign[ii]+agedata$Interval[1])] ###loop over and fill in the rest. Since survival rates are 5 year they need to be aggregated across the interval with the prodcuct
}
cumsurv<-rev(cumprod(rev(srateint))) #has to be double reversed for cumprod to do the accumulation in the right direction, seems like the function could use a reverse option
agedata$Bstar<-agedata$B/cumsurv[-1] ###Correcting B based on cumulative survival of cohort from establishment to observation. Drop fist value which would be cumulative survival of cohort before the first est year
N<-0 ###starting population size, loop over intervals
for (t in 1:length(agedata$B)) {
N<-agedata$Bstar[t]+N*srateint[t] # population size calculation
agedata$Nstar[t]<-N
}
agedata$grate<-c(NA,diff(log(agedata$Nstar))/(agedata$Interval[-1]/agint)) #10 year growth rate on log scale corrected for interval length
agedata$brate<-exp(agedata$grate)-(srateint[-length(srateint)]^(1/(agedata$Interval[1]/agint)))#Establishment rate calculation. Since grate is now on a 10 year time period for all, use 10 year survival rate not srateint
##Helpful for plugging results into containers
yrs<-dataout$Time%in%agedata$Talign
loc<-which(dataout$dataset==i & yrs)
###Plug results into container###
dataout$grate[loc]<-exp(agedata$grate)
dataout$brate[loc]<-agedata$brate
dataout$srate[loc]<-srateint[-length(srateint)]^(1/(agedata$Interval[1]/agint))
dataout$B[loc]<-agedata$B
dataout$Nstar[loc]<-agedata$Nstar
dataout$Bstar[loc]<-agedata$Bstar
dataout$Relpop[loc]<-agedata$Nstar/max(agedata$Nstar)
dataout$Relt_1[loc]<-c(NA,agedata$Nstar[-length(agedata$B)])/max(agedata$Nstar)
dataout$RelB[loc]<-agedata$Bstar/sum(agedata$Bstar)
dataout$deltabrate[loc]<-c(NA,diff(agedata$brate))
dataout$deltaB[loc]<-c(NA,diff(agedata$Bstar/sum(agedata$Bstar)))
dataout$deltaN[loc]<-c(NA,diff(agedata$Nstar/max(agedata$Nstar)))
dataout$Species[loc]<-agedata$Species
}
return(dataout)
}
par(mfrow=c(2,1),mar = c(0.5,6,4.5,.5))
plot(0,0,ylim=c(0,1),xlim=c(1600,1990),xaxt="n", xlab='', ylab=expression("Survival rate (20yrs"^-1~")"))
rect(0, -1000, 1850, 1000, col=adjustcolor("grey",0.3 ), border='white',lwd=2)
rect(1850, -1000, 2050, 1000, col=adjustcolor("coral", 0.2), border='white',lwd=2)
set.seed(100)
for(i in 1:50){
sv<-svariable(x=agedataall20,0.98,backward = F)
sv_mean<-na.omit(decadeavg(sv,var='srate'))
lines(sv_mean$years, sv_mean$mean,lwd=.2,col='steelblue')
sv<-svariable(x=agedataall20,0.98,backward = T)
sv_mean<-na.omit(decadeavg(sv,var='srate'))
lines(sv_mean$years, sv_mean$mean,lwd=.2,col='steelblue')
}
fig_label("A",region="plot", cex=2)
par(mar = c(4.5,6,.5,.5))
plot(0,0,ylim=c(0,.7),xlim=c(1600,1990), ylab=expression("Establishment rate (20yrs"^-1~")"), xlab='Year')
rect(0, -1000, 1850, 1000, col=adjustcolor("grey",0.3 ), border='white',lwd=2)
rect(1850, -1000, 2050, 1000, col=adjustcolor("coral", 0.2), border='white',lwd=2)
text(1730,0.65,substitute(paste(bold('Pre E-A Settlement'))))
text(1940,0.65,substitute(paste(bold('Post E-A Settlement'))))
set.seed(100) # for reproducable results
for(i in 1:50){
sv<-svariable(x=agedataall20,0.98,backward = F)
sv_mean<-na.omit(decadeavg(sv,var='brate'))
lines(sv_mean$years, sv_mean$mean,lwd=.2, col='steelblue')
sv<-svariable(x=agedataall20,0.98,backward = T)
sv_mean<-na.omit(decadeavg(sv,var='brate'))
lines(sv_mean$years, sv_mean$mean,lwd=.2,col='steelblue')
}
lines(s1avg$years,s1avg$mean, lwd=2,lty=2)
s.90<-sconstant(agedataall20,srate=.90)
s.90avg<-mvavg(s.90,smoothyears = 20)
lines(s.90avg$years,s.90avg$mean, lwd=2,lty=2, col='coral')
fig_label("B",region="plot", cex=2)
svariable<-function(x,sstart,backward=T,agint=20){ ###svec is a vector of survival values from the beginning to end
TimeSh<-seq(min(x$Talign),2020,by=20)
Time<-rep(TimeSh,max(x$Dataset))
srateall<-rep(NA,length(TimeSh))
s<-logit(sstart)
for(i in 1:length(TimeSh)){s<-(s+rnorm(1,0,0.45))
srateall[i]<-inv.logit(s)}
if(backward==T){svec<-rev(srateall)}
if(backward==F){svec<-(srateall)}
###Create containers for output
dataout<-as.data.frame(Time)
dataout$dataset<-(rep(1:max(x$Dataset), each=length(TimeSh)))
dataout$grate<-NA
dataout$brate<-NA
dataout$srate<-NA
dataout$B<-NA
dataout$Nstar<-NA
dataout$Bstar<-NA
dataout$Relpop<-NA
dataout$Relt_1<-NA
dataout$RelB<-NA
dataout$Species<-NA
dataout$deltabrate<-NA
dataout$deltaB<-NA
dataout$deltaN<-NA
####Loop over all datasets####
for (i in 1:max(x$Dataset)){
agedata<-x[which(x$Dataset==i),]#extract individual dataset
agedata$firstsurvival<-min(agedata$CensusOffset)/agedata$Interval[1] ###Correction needed for offset between decade collected and last decade of observed establishment
i10<-(agedata$firstsurvival[1]*agedata$Interval[1])
if (i10<=agint) {surv1<- svec[which(TimeSh==max(agedata$Talign)+agedata$Interval[1])]^(i10/agint)}
if (i10>agint) { wsteps<-floor(i10/agint) ###Whole time steps missing in 5 year periods
psteps<-(i10/agint)-wsteps ###partial time steps missing
steps<-ceiling(i10/agint) ##Total number of steps, rounded up
surv1<-prod(svec[(which(TimeSh==max(agedata$Talign)+agedata$Interval[1])):(which(TimeSh==max(agedata$Talign)+agedata$Interval[1])+wsteps-1)])*(svec[(which(TimeSh==max(agedata$Talign)+agedata$Interval[1])+steps-1)]^(psteps)) ##survival from the last establishment cohort to census
}
srateint<-numeric(length(agedata$B)+1)# Create vector of survival values for each interval. It's 1 longer than the number of intervals to include survival from the last cohort to census
srateint[length(agedata$B)+1]<-surv1 ###last slot is survival from the last cohort to census
for(ii in (length(agedata$B)):1){
srateint[ii]<-svec[which(TimeSh>=agedata$Talign[ii] & TimeSh<agedata$Talign[ii]+agedata$Interval[1])] ###loop over and fill in the rest. Since survival rates are 5 year they need to be aggregated across the interval with the prodcuct
}
cumsurv<-rev(cumprod(rev(srateint))) #has to be double reversed for cumprod to do the accumulation in the right direction, seems like the function could use a reverse option
agedata$Bstar<-agedata$B/cumsurv[-1] ###Correcting B based on cumulative survival of cohort from establishment to observation. Drop fist value which would be cumulative survival of cohort before the first est year
N<-0 ###starting population size, loop over intervals
for (t in 1:length(agedata$B)) {
N<-agedata$Bstar[t]+N*srateint[t] # population size calculation
agedata$Nstar[t]<-N
}
agedata$grate<-c(NA,diff(log(agedata$Nstar))/(agedata$Interval[-1]/agint)) #10 year growth rate on log scale corrected for interval length
agedata$brate<-exp(agedata$grate)-(srateint[-length(srateint)]^(1/(agedata$Interval[1]/agint)))#Establishment rate calculation. Since grate is now on a 10 year time period for all, use 10 year survival rate not srateint
##Helpful for plugging results into containers
yrs<-dataout$Time%in%agedata$Talign
loc<-which(dataout$dataset==i & yrs)
###Plug results into container###
dataout$grate[loc]<-exp(agedata$grate)
dataout$brate[loc]<-agedata$brate
dataout$srate[loc]<-srateint[-length(srateint)]^(1/(agedata$Interval[1]/agint))
dataout$B[loc]<-agedata$B
dataout$Nstar[loc]<-agedata$Nstar
dataout$Bstar[loc]<-agedata$Bstar
dataout$Relpop[loc]<-agedata$Nstar/max(agedata$Nstar)
dataout$Relt_1[loc]<-c(NA,agedata$Nstar[-length(agedata$B)])/max(agedata$Nstar)
dataout$RelB[loc]<-agedata$Bstar/sum(agedata$Bstar)
dataout$deltabrate[loc]<-c(NA,diff(agedata$brate))
dataout$deltaB[loc]<-c(NA,diff(agedata$Bstar/sum(agedata$Bstar)))
dataout$deltaN[loc]<-c(NA,diff(agedata$Nstar/max(agedata$Nstar)))
dataout$Species[loc]<-agedata$Species
}
return(dataout)
}
par(mfrow=c(2,1),mar = c(0.5,6,4.5,.5))
plot(0,0,ylim=c(0,1),xlim=c(1600,1990),xaxt="n", xlab='', ylab=expression("Survival rate (20yrs"^-1~")"))
rect(0, -1000, 1850, 1000, col=adjustcolor("grey",0.3 ), border='white',lwd=2)
rect(1850, -1000, 2050, 1000, col=adjustcolor("coral", 0.2), border='white',lwd=2)
set.seed(100)
for(i in 1:50){
sv<-svariable(x=agedataall20,0.98,backward = F)
sv_mean<-na.omit(decadeavg(sv,var='srate'))
lines(sv_mean$years, sv_mean$mean,lwd=.2,col='steelblue')
sv<-svariable(x=agedataall20,0.98,backward = T)
sv_mean<-na.omit(decadeavg(sv,var='srate'))
lines(sv_mean$years, sv_mean$mean,lwd=.2,col='steelblue')
}
fig_label("A",region="plot", cex=2)
par(mar = c(4.5,6,.5,.5))
plot(0,0,ylim=c(0,.7),xlim=c(1600,1990), ylab=expression("Establishment rate (20yrs"^-1~")"), xlab='Year')
rect(0, -1000, 1850, 1000, col=adjustcolor("grey",0.3 ), border='white',lwd=2)
rect(1850, -1000, 2050, 1000, col=adjustcolor("coral", 0.2), border='white',lwd=2)
text(1730,0.65,substitute(paste(bold('Pre E-A Settlement'))))
text(1940,0.65,substitute(paste(bold('Post E-A Settlement'))))
set.seed(100) # for reproducable results
for(i in 1:50){
sv<-svariable(x=agedataall20,0.98,backward = F)
sv_mean<-na.omit(decadeavg(sv,var='brate'))
lines(sv_mean$years, sv_mean$mean,lwd=.2, col='steelblue')
sv<-svariable(x=agedataall20,0.98,backward = T)
sv_mean<-na.omit(decadeavg(sv,var='brate'))
lines(sv_mean$years, sv_mean$mean,lwd=.2,col='steelblue')
}
lines(s1avg$years,s1avg$mean, lwd=2,lty=2)
s.90<-sconstant(agedataall20,srate=.90)
s.90avg<-mvavg(s.90,smoothyears = 20)
lines(s.90avg$years,s.90avg$mean, lwd=2,lty=2, col='coral')
fig_label("B",region="plot", cex=2)
par(mfrow=c(2,1),mar = c(0.5,6,4.5,.5))
plot(0,0,ylim=c(0,1),xlim=c(1600,1990),xaxt="n", xlab='', ylab=expression("Survival rate (20yrs"^-1~")"))
rect(0, -1000, 1850, 1000, col=adjustcolor("grey",0.3 ), border='white',lwd=2)
rect(1850, -1000, 2050, 1000, col=adjustcolor("coral", 0.2), border='white',lwd=2)
set.seed(100)
for(i in 1:50){
sv<-svariable(x=agedataall20,0.95,backward = F)
sv_mean<-na.omit(decadeavg(sv,var='srate'))
lines(sv_mean$years, sv_mean$mean,lwd=.2,col='steelblue')
sv<-svariable(x=agedataall20,0.95,backward = T)
sv_mean<-na.omit(decadeavg(sv,var='srate'))
lines(sv_mean$years, sv_mean$mean,lwd=.2,col='steelblue')
}
fig_label("A",region="plot", cex=2)
par(mar = c(4.5,6,.5,.5))
plot(0,0,ylim=c(0,.7),xlim=c(1600,1990), ylab=expression("Establishment rate (20yrs"^-1~")"), xlab='Year')
rect(0, -1000, 1850, 1000, col=adjustcolor("grey",0.3 ), border='white',lwd=2)
rect(1850, -1000, 2050, 1000, col=adjustcolor("coral", 0.2), border='white',lwd=2)
text(1730,0.65,substitute(paste(bold('Pre E-A Settlement'))))
text(1940,0.65,substitute(paste(bold('Post E-A Settlement'))))
set.seed(100) # for reproducable results
for(i in 1:50){
sv<-svariable(x=agedataall20,0.95,backward = F)
sv_mean<-na.omit(decadeavg(sv,var='brate'))
lines(sv_mean$years, sv_mean$mean,lwd=.2, col='steelblue')
sv<-svariable(x=agedataall20,0.95,backward = T)
sv_mean<-na.omit(decadeavg(sv,var='brate'))
lines(sv_mean$years, sv_mean$mean,lwd=.2,col='steelblue')
}
lines(s1avg$years,s1avg$mean, lwd=2,lty=2)
s.90<-sconstant(agedataall20,srate=.90)
s.90avg<-mvavg(s.90,smoothyears = 20)
lines(s.90avg$years,s.90avg$mean, lwd=2,lty=2, col='coral')
fig_label("B",region="plot", cex=2)
srateint<-numeric(length(agedata$B)+1)# Create vector of survival values for each interval. It's 1 longer than the number of intervals to include survival from the last cohort to census
srateint[length(agedata$B)+1]<-surv1 ###last slot is survival from the last cohort to census
for(ii in (length(agedata$B)):1){
srateint[ii]<-svec[which(TimeSh>=agedata$Talign[ii] & TimeSh<agedata$Talign[ii]+agedata$Interval[1])] ###loop over and fill in the rest. Since survival rates are 5 year they need to be aggregated across the interval with the prodcuct
}
srateint
rev(cumprod(rev(srateint)))
agedata$B
agedata$Bstar<-agedata$B/cumsurv[-1]
cumsurv<-rev(cumprod(rev(srateint))) #has to be double reversed for cumprod to do the accumulation in the right direction, seems like the function could use a reverse option
agedata$Bstar<-agedata$B/cumsurv[-1] ###Correcting B based on cumulative survival of cohort from establishment to observation. Drop fist value which would be cumulative survival of cohort before the first est year
plot(agedata$Bstar)
lines(agedata$B)
agedata$Bstar
agedata$B
assigning parameters
r <- 0.1 #growth rate
K <- 300 #carrying capacity
initialN <- 2 # initial population size
# incorporate time and set an N vector
time <- seq(1:100) #Time
N <-  rep(NA,length(time)) #population size
names(N) <- time
N
N[1] <- initialN
#Create logistic growth function
loggrowth <- function(r,K,N){
final_result = N + r*N*(1-N/K)
return(final_result)
}
#incorporate logistic growth function in for loop
i=1
for (i in 2:length(time)){
N[i] = loggrowth(r=r,K=K,N=N[i-1])
}
#for loop without logistic growth function
i=1
for (i in 2:length(time)){
N[i] = N[i-1] + r*N[i-1]*(1-N[i-1]/K)
}
N
round(N)
#Create graph
plot(N~time)
plot(N~time, type = "l", xlab = "time (years)",
ylab = "population size", main = "logistic growth curve")
N
round(N)
## Ecological Forecasting - Bob Shriver
## Melissa Schwan
# Density Dependent Population Model Lab ----
## 1. Assign values to 'r' and 'K' ----
r = 0.1 # growth rate
K = 500 # carrying capacity (max population)
N_init = 3 # initial population
## 2. Create 'time' vector using seq() ----
time = seq(1:90)
## 3. Create 'N' vector using rep() ----
N = rep(NA,length(time)) # initialize population vector of NAs for each time point in 'time'
N[1] = N_init # initial population
## 4. Logistic Growth function ----
# inputs:
# r, growth rate
# K, max pop, carrying capacity
# N_init, initial population
LogisticGrowth <- function(N_init, r, K){
N_predict = N_init + r*N_init*(1-N_init/K)
return(N_predict)
}
## 5. "For" loop that steps through time, calculates population for each time point based on the previous population size, using the LogisticGrowth() function ----
# neat indexing trick: time[-1] removes the first value of time
for(t in time[-1]){
N[t] = LogisticGrowth(N_init = N[t-1], r = r, K = K)
}
# print(N)
## 6. Plot population time series using plot() ----
plot(time, N, main = "Logistic Pop Growth Model", type = "l")
## Lab 1 DD population model
## Griffin Shelor
## assigning values to r and K, creating time vector, creating N vector
r <- 0.2
K <- 500
time_vector <- seq(1:100)
N <- rep(0, 100)
N[1] <- 3
## for loop
for (i in 2:length(time_vector)) {
N[i] = N[i - 1] + r * N[i - 1] * (1 - N[i - 1]/K)
}
## plotting population over time
plot(x = time_vector, y = N)
## trying a function
logistic <- function(n, R, k) {
new_n <- n + R * n * (1 - n / k)
return(new_n)
}
## for loop with function representing logistic growth equation
for (i in 2:length(time_vector)) {
N[i] <- logistic(n = N[i - 1], R = r, k = K)
}
plot(x = time_vector, y = N, main = "Functionalized Model")
1125*1.5
######
##Timeseries decomp  (adapted from Peter Adler)
#####
library(forecast)
library(stlplus)
library(fpp)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))##set working directory to current file
## Inherent scales within a time series:
#  Often our data has some frequency of collection within a year (e.g. daily, monthly).
#  We might be interested in either focusing on that scale, or removing the effects of it.
#  For example - seasonally adjusted housing sales or unemployment.
## Time series decomposition:
# a time series approach for trying to pull out the signals at different scales.
# Breaks down a time series into the trend, seasonal, and "irregular" fluctuations
# Use example of atmospheric CO2.
# To extract these components, there are generally 3 basic steps.
# 1) we fit something to the observed data to extract the long-term trend
# 2) we fit a seasonal model to the remaining data to pull out the seasonal signal
# 3) whatever is left over are the irregular fluctuations (residuals)
## Time Series Objects
# To do a time series decomp using existing packages, we need our data to be a time series object.
# This is a data format, like a dataframe is a format, that has a special structure and R
# knows to work with it in a special way.
# Some packages will require you to put your data into a time series object specific to that
# package. For today, we will use the standard ts object in the base package. It's limitation is
# that it can only take regularly spaced data (i.e. monthly, daily, quarterly, annual). There are
# other methods that can take irregular data and import that into a time series object. Packages
# that can handle irregular data include zoo and xts.
## Example: ts
# Exploring  patterns with decomposition using NDVI
# need to make sure your data is already in chronological order
NDVI = read.csv('./../data/portal_timeseries.csv', stringsAsFactors = FALSE)
head(NDVI)
NDVI.ts = ts(NDVI$NDVI, start = c(1992, 3), end = c(2014, 11), frequency = 12)
plot(NDVI.ts, xlab = "Year", ylab="greenness")
class(NDVI.ts)
NDVI.ts
start(NDVI.ts)
end(NDVI.ts)
# You cannot slice and dice a ts object without losing the date info, unless you
# use a special tool
str(NDVI.ts)
data.2000 = window(NDVI.ts, start=c(1999,1),end=c(2000,12))
data.2000
fit_add = decompose(NDVI.ts, type = 'additive')
plot(fit_add)
str(fit_add)
#What is the moving average window, hard to tell based on function documentation and output
fit_mult = decompose(NDVI.ts, type = 'multiplicative')
plot(fit_mult)
str(fit_mult)
# It's hard to see the seasonal pattern, so let's zoom in.
# by subsetting the seasonal fits:
plot(fit_add$seasonal[11:23],type="o") # started at 11 b/c it is the first January
# Would we get the same answer just by calculating monthly means?
# Use tapply() and cycle():
monthly_means <- tapply(NDVI.ts, cycle(NDVI.ts), FUN=mean)
plot(monthly_means,type="o")
#Autocorrelation (adapted from Peter Adler)
#####
rm(list=ls())
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))##set working directory to current file
library(forecast)
library(astsa)
library(tseries)
# Let's start by thinking about an extreme case of a time series. Say I have a normal distribution
# centered on zero and at each time step I randomly draw a value from the distribution
# What does my time series look like? (highly variable, no trend).
# We can simulate this random time series where the value at one time step has
# no dependence on the previous time step. This gives us a signal that we typically
# call white noise.
set.seed(20)
whitenoise <- ts(rnorm(273))
plot(whitenoise, main="White noise")
# rnorm generates a vector of numbers randomly drawn from a normal distribution
# with mean = 0 and sd = 1
# Is this how we would expect a normal biological time series to operate? (No. the value
# at t+1 probably has some dependence on the value at t)
data = read.csv('./../data/portal_timeseries.csv')
head(data)
NDVI.ts = ts(data$NDVI, start = c(1992, 3), end = c(2014, 11), frequency = 12)
plot(NDVI.ts, xlab = "Year", ylab="greenness", main="NDVI")
# clearly the biological time series and the white noise series look different
# from white noise, we know that the NDVI data has seasonality in the signal, so
# that's clearly one major difference, related to that is that the value at one
# time step is not necessarily independent from previous time steps. We can
# explore that dependence by looking at lag plots. How much does one time step
# tell you about the next?
# Lag plots are simply the correlation between values at time t and some time step in
# the past. The difference between time t and a value in the past is the lag.
# So here, let's plot all the lags up to 12 months.
lag.plot(NDVI.ts, lags=12, do.lines=FALSE)
# The lag.plot is showing you the autocorrelation within a time series. Which
# is great as a data viz step, but hard to interpret more precisely than to see
# how the time series is related to itself and when things look more strongly related
# At each one of these lags, we could calculated a correlation coefficient between the data at
# time 2 and some number of lags in the past.
# An autocorrelation function (ACF) or correlogram conducts those correlations  and plots them
# in a way that makes the autocorrelation structure of the time series easier to understand
acf(NDVI.ts)
# A time series should always give you a correlation coefficient of 1 at a zero time lag, depending
# on the package you use, you may or may not get the 0 lag coefficient.So just make sure you check before
# you get excited about having a strong signal
# In this case, the x axis is the proportion of the annual frequency
# for the NDVI time series, but sometimes this is displayed in the actual units of the timeseries (months in our case)
# For our NDVI time series, the strongest correlation is at a lag of 1 month, and then the
# autocorrelation in the time series drop off substantially. We get little blips every 12
# months which is the seasonality coming through and perhaps a weak signal of a negative relationship
# at 6 month lags. What might that mean, biologically?
# the blue lines plotted are the 95% confidence interval for the autocrrelation of
# a time series (+- 1.96/(sqrt(T))). If more than 5% of spikes are outside this bound,
# your time series is probably not white noise.
lag.plot(whitenoise, lag = 12, do.lines = FALSE)
acf(whitenoise)
# If there's a pattern to your spikes,
# that is usually another good sign that you have autocorrelation structure in your
# time series
# Let's look at a couple more examples
PPT.ts = ts(data$rain, start=c(1992,3), end = c(2014,11), frequency=12)
lag.plot(PPT.ts, lags=12, do.lines=FALSE)
acf(PPT.ts)
rats.ts = ts(data$rodents, start=c(1992,3), end = c(2014,11), frequency=12)
lag.plot(rats.ts, lags=12, do.lines=FALSE)
acf(rats.ts)
# Autocorrelation can echo through a time series. If Y_t and Y_t-1 are strongly correlated
# and Y_t-1 and Y_t-2 are strongly correlated then presumably Y_t and Y_t-2 are correlated
# even if there is no causal factor involved. We can examine this by using a partial acf. In fact
# the correlation at lag2 is the square of the correlation at lag 1.
# pacf takes this relationship into account and gives you the correlation coefficient between t and t-2 once
# you account for the relationship between t and t-1.
acf(NDVI.ts)
pacf(NDVI.ts) # note pacf starts at lag 1 not 0
# Forecast package has a nice feature that let's you look at the time series, the acf, and pacf at the same time
tsdisplay(NDVI.ts)
tsdisplay(rats.ts)
tsdisplay(PPT.ts)
# The rodents show a classic signal of an autoregressive model. What is an autoregressive model?
# Something where the value at time t depends on the values at previous time steps.
# A classic autoregressive model is a random walk:
set.seed(1)
x = w = rnorm(1000)
for (t in 2:1000) x[t] = x[t-1]+w[t]
tsdisplay(x)
# you can use these plots to get a better understanding of your time series. What dynamics are occurring?
# What questions might be worth asking or what is surprising to you that doesn't seem to be coming through?
# Understanding your autocorrelation structure is also important statistically
# Regression approaches with autocorrelated data will give underestimates of the variance
# inflated test statistics, and narrow CIs.
# Finally, sometimes you want to understand how two time series are correlated
# across different lags. You can use the cross-correlation function to dig into that
ccf.plantsrain = ccf(PPT.ts, NDVI.ts)
plot(ccf.plantsrain)
lag2.plot(PPT.ts, NDVI.ts, 12)
ccf.plantrat = ccf(NDVI.ts, rats.ts)
plot(ccf.plantrat)
lag2.plot(NDVI.ts, rats.ts, 12)
# Some take home messages for autocorrelation:
# 1. Autocorrelation is useful in that information about the past and future states
#    of the system are encoded in the timeseries. This is information that can
#    potentially be leveraged for forecasting.
# 2. Autocorrelation can be  a statistical pain. Statistical approaches
#    Assume iid: independent and identically distributed errors. i.e. that your
#    data is a random draw from an underlying distribution. But autocorrelation
#    means that your data is not a random draw. Each draw is influenced by the
#    previous draw. This means that if you put a time series through a regression
#    your confidence intervals will be smaller than they should be. Parameter
#    estimates are (often) ok. Need to deal with that autocorrelation for statsitical tests
#    many modern R approaches have a method for dealing or specifying autocorrelated
#   errors - sometimes referred to as covariance in the errors.
# Stationarity: mathematically no moment of the distribution for the time series depends
# upon or changes predictably with time.
# Practically, constant mean, variance, and autocovariance does not depend on time
# Can check to see if your data is
adf.test(whitenoise)
adf.test(x, alternative = "stationary")
adf.test(NDVI.ts)
adf.test(PPT.ts)
adf.test(rats.ts)
# if p value > 0.05, the time series is not stationary
# if p value < 0.05, time series is stationary
1124*1.5
1686*.03
